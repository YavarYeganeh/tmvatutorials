{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://oproject.org/img/ROOT.png\" height=\"30%\" width=\"30%\">\n",
    "<img src=\"http://oproject.org/img/tmvalogo.png\" height=\"30%\" width=\"30%\">\n",
    "\n",
    "<hr style=\"border-top-width: 4px; border-top-color: #34609b;\">\n",
    "\n",
    "# TMVA Classification Example Using a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Factory\n",
    "\n",
    "Create the Factory class. Later you can choose the methods\n",
    "whose performance you'd like to investigate. \n",
    "\n",
    "The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass\n",
    "\n",
    " - The first argument is the base of the name of all the output\n",
    "weightfiles in the directory weight/ that will be created with the \n",
    "method parameters \n",
    "\n",
    " - The second argument is the output file for the training results\n",
    "  \n",
    " - The third argument is a string option defining some general configuration for the TMVA session. For example all TMVA output can be suppressed by removing the \"!\" (not) in front of the \"Silent\" argument in the option string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TMVA::Tools::Instance();\n",
    "\n",
    " // for using Keras\n",
    "gSystem->Setenv(\"KERAS_BACKEND\",\"tensorflow\"); \n",
    "TMVA::PyMethodBase::PyInitialize();\n",
    "\n",
    "\n",
    "\n",
    "auto outputFile = TFile::Open(\"CNN_ClassificationOutput.root\", \"RECREATE\");\n",
    "\n",
    "TMVA::Factory factory(\"TMVA_CNN_Classification\", outputFile,\n",
    "                      \"!V:ROC:!Silent:Color:!DrawProgressBar:AnalysisType=Classification\" ); \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare DataLoader(s)\n",
    "\n",
    "The next step is to declare the DataLoader class that deals with input variables \n",
    "\n",
    "Define the input variables that shall be used for the MVA training\n",
    "note that you may also use variable expressions, which can be parsed by TTree::Draw( \"expression\" )]\n",
    "\n",
    "In this case the input data consists of an image of 16x16 pixels. Each single pixel is a branch in a ROOT TTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TMVA::DataLoader * loader = new TMVA::DataLoader(\"dataset\");\n",
    "\n",
    "int imgSize = 8 * 8; \n",
    "for(auto i = 0; i < imgSize; i++)\n",
    " {\n",
    "     loader->AddVariable(Form(\"var%d\",i),'F');\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dataset(s)\n",
    "\n",
    "Define input data file and signal and background trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataSetInfo              : [dataset] : Added class \"Signal\"\n",
      "                         : Add Tree sig_tree of type Signal with 10000 events\n",
      "DataSetInfo              : [dataset] : Added class \"Background\"\n",
      "                         : Add Tree bkg_tree of type Background with 10000 events\n"
     ]
    }
   ],
   "source": [
    "TString inputFileName = \"http://oproject.org/data/images_data.root\";\n",
    "\n",
    "//TString inputFileName = \"tmva_class_example.root\";\n",
    "\n",
    "auto inputFile = TFile::Open( inputFileName );\n",
    "\n",
    "// --- Register the training and test trees\n",
    "\n",
    "TTree *signalTree     = (TTree*)inputFile->Get(\"sig_tree\");\n",
    "TTree *backgroundTree = (TTree*)inputFile->Get(\"bkg_tree\");\n",
    "\n",
    "// global event weights per tree (see below for setting event-wise weights)\n",
    "Double_t signalWeight     = 1.0;\n",
    "Double_t backgroundWeight = 1.0;\n",
    "   \n",
    "// You can add an arbitrary number of signal or background trees\n",
    "loader->AddSignalTree    ( signalTree,     signalWeight     );\n",
    "loader->AddBackgroundTree( backgroundTree, backgroundWeight );\n",
    "\n",
    "\n",
    "// Set individual event weights (the variables must exist in the original TTree)\n",
    "//    for signal    : factory->SetSignalWeightExpression    (\"weight1*weight2\");\n",
    "//    for background: factory->SetBackgroundWeightExpression(\"weight1*weight2\");\n",
    "//loader->SetBackgroundWeightExpression( \"weight\" );\n",
    "\n",
    "// Apply additional cuts on the signal and background samples (can be different)\n",
    "TCut mycuts = \"\"; // for example: TCut mycuts = \"abs(var1)<0.5 && abs(var2-0.5)<1\";\n",
    "TCut mycutb = \"\"; // for example: TCut mycutb = \"abs(var1)<0.5\";\n",
    "\n",
    "// Tell the factory how to use the training and testing events\n",
    "//\n",
    "// If no numbers of events are given, half of the events in the tree are used \n",
    "// for training, and the other half for testing:\n",
    "//    loader->PrepareTrainingAndTestTree( mycut, \"SplitMode=random:!V\" );\n",
    "// To also specify the number of testing events, use:\n",
    "\n",
    "loader->PrepareTrainingAndTestTree( mycuts, mycutb,\n",
    "                                    \"nTrain_Signal=0:nTrain_Background=0:SplitMode=Random:NormMode=NumEvents:!V\" );\n",
    "\n",
    "\n",
    "\n",
    "//loader->PrepareTrainingAndTestTree(mycuts, mycutb,\n",
    "//                                   \"nTrain_Signal=5000:nTrain_Background=5000:nTest_Signal=5000:nTest_Background=5000:SplitMode=Random:NormMode=NumEvents:!V\" ); \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Booking Methods\n",
    "\n",
    "Here we book the TMVA methods. We book a Likelihood based on KDE, a Fischer discriminant and a BDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : Booking method: \u001b[1mBDT\u001b[0m\n",
      "                         : \n",
      "DataSetFactory           : [dataset] : Number of events in input trees\n",
      "                         : \n",
      "                         : \n",
      "                         : Number of training and testing events\n",
      "                         : ---------------------------------------------------------------------------\n",
      "                         : Signal     -- training events            : 5000\n",
      "                         : Signal     -- testing events             : 5000\n",
      "                         : Signal     -- training and testing events: 10000\n",
      "                         : Background -- training events            : 5000\n",
      "                         : Background -- testing events             : 5000\n",
      "                         : Background -- training and testing events: 10000\n",
      "                         : \n"
     ]
    }
   ],
   "source": [
    "\n",
    "//Boosted Decision Trees\n",
    "factory.BookMethod(loader,TMVA::Types::kBDT, \"BDT\",\n",
    "                   \"!V:NTrees=800:MinNodeSize=2.5%:MaxDepth=2:BoostType=AdaBoost:AdaBoostBeta=0.5:UseBaggedBoost:BaggedSampleFraction=0.5:SeparationType=GiniIndex:nCuts=20\" );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booking Deep Neural Network\n",
    "\n",
    "Here we book the new DNN of TMVA. If using master version you can use the new DL method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bool useDNN = true; \n",
    "bool useCNN = true; \n",
    "bool useKeras = false; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : Booking method: \u001b[1mDNN_CPU\u001b[0m\n",
      "                         : \n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:Layout=TANH|64,TANH|64,TANH|64,LINEAR:TrainingStrategy=LearningRate=1e-1,Momentum=0.9,Repetitions=1,ConvergenceSteps=20,BatchSize=32,TestRepetitions=5,WeightDecay=1e-4,Regularization=L2,DropConfig=0.0+0.2+0.2+0.:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     <none>\n",
      "                         : - Default:\n",
      "                         :     Boost_num: \"0\" [Number of times the classifier will be boosted]\n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:Layout=TANH|64,TANH|64,TANH|64,LINEAR:TrainingStrategy=LearningRate=1e-1,Momentum=0.9,Repetitions=1,ConvergenceSteps=20,BatchSize=32,TestRepetitions=5,WeightDecay=1e-4,Regularization=L2,DropConfig=0.0+0.2+0.2+0.:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     V: \"True\" [Verbose output (short form of \"VerbosityLevel\" below - overrides the latter one)]\n",
      "                         :     VarTransform: \"None\" [List of variable transformations performed before training, e.g., \"D_Background,P_Signal,G,N_AllClasses\" for: \"Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)\"]\n",
      "                         :     H: \"False\" [Print method-specific help message]\n",
      "                         :     Layout: \"TANH|64,TANH|64,TANH|64,LINEAR\" [Layout of the network.]\n",
      "                         :     ErrorStrategy: \"CROSSENTROPY\" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]\n",
      "                         :     WeightInitialization: \"XAVIERUNIFORM\" [Weight initialization strategy]\n",
      "                         :     Architecture: \"CPU\" [Which architecture to perform the training on.]\n",
      "                         :     TrainingStrategy: \"LearningRate=1e-1,Momentum=0.9,Repetitions=1,ConvergenceSteps=20,BatchSize=32,TestRepetitions=5,WeightDecay=1e-4,Regularization=L2,DropConfig=0.0+0.2+0.2+0.\" [Defines the training strategies.]\n",
      "                         : - Default:\n",
      "                         :     VerbosityLevel: \"Default\" [Verbosity level]\n",
      "                         :     CreateMVAPdfs: \"False\" [Create PDFs for classifier outputs (signal and background)]\n",
      "                         :     IgnoreNegWeightsInTraining: \"False\" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]\n",
      "                         :     ValidationSize: \"20%\" [Part of the training data to use for validation. Specify as 0.2 or 20% to use a fifth of the data set as validation set. Specify as 100 to use exactly 100 events. (Default: 20%)]\n"
     ]
    }
   ],
   "source": [
    "if (useDNN) { \n",
    "    \n",
    "     TString layoutString (\"Layout=TANH|64,TANH|64,TANH|64,LINEAR\");\n",
    "//                                                                                                                                                                                       \n",
    "      // Training strategies \n",
    "      // one can catenate several training strategies \n",
    "      TString training1(\"LearningRate=1e-2,Momentum=0.9,Repetitions=1,\"\n",
    "                        \"ConvergenceSteps=20,BatchSize=32,TestRepetitions=5,\"\n",
    "                        \"WeightDecay=1e-4,Regularization=L2,\"\n",
    "                        \"DropConfig=0.0+0.2+0.2+0.\");\n",
    "  \n",
    "      TString trainingStrategyString (\"TrainingStrategy=\");\n",
    "      trainingStrategyString += training1; // + \"|\" + training2 + \"|\" + training3;\n",
    "\n",
    "      // General Options.                                                                                                                                                                \n",
    "      TString dnnOptions (\"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:\"\n",
    "                          \"WeightInitialization=XAVIERUNIFORM\");\n",
    "      dnnOptions.Append (\":\"); dnnOptions.Append (layoutString);\n",
    "      dnnOptions.Append (\":\"); dnnOptions.Append (trainingStrategyString);\n",
    "\n",
    "      dnnOptions += \":Architecture=CPU\";\n",
    "      factory.BookMethod(loader, TMVA::Types::kDNN, \"DNN_CPU\", dnnOptions);\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book Convolutional Neural Network in TMVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : Booking method: \u001b[1mDL_CNN_CPU\u001b[0m\n",
      "                         : \n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:InputLayout=1|8|8:BatchLayout=256|1|64:Layout=CONV|10|3|3|1|1|1|1|RELU,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|64|TANH,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-1,Momentum=0.9,Repetitions=1,ConvergenceSteps=20,BatchSize=256,TestRepetitions=5,WeightDecay=1e-4,Regularization=None,DropConfig=0.0+0.5+0.5+0.5, Multithreading=False:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     <none>\n",
      "                         : - Default:\n",
      "                         :     Boost_num: \"0\" [Number of times the classifier will be boosted]\n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:InputLayout=1|8|8:BatchLayout=256|1|64:Layout=CONV|10|3|3|1|1|1|1|RELU,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|64|TANH,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-1,Momentum=0.9,Repetitions=1,ConvergenceSteps=20,BatchSize=256,TestRepetitions=5,WeightDecay=1e-4,Regularization=None,DropConfig=0.0+0.5+0.5+0.5, Multithreading=False:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     V: \"True\" [Verbose output (short form of \"VerbosityLevel\" below - overrides the latter one)]\n",
      "                         :     VarTransform: \"None\" [List of variable transformations performed before training, e.g., \"D_Background,P_Signal,G,N_AllClasses\" for: \"Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)\"]\n",
      "                         :     H: \"False\" [Print method-specific help message]\n",
      "                         :     InputLayout: \"1|8|8\" [The Layout of the input]\n",
      "                         :     BatchLayout: \"256|1|64\" [The Layout of the batch]\n",
      "                         :     Layout: \"CONV|10|3|3|1|1|1|1|RELU,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|64|TANH,DENSE|1|LINEAR\" [Layout of the network.]\n",
      "                         :     ErrorStrategy: \"CROSSENTROPY\" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]\n",
      "                         :     WeightInitialization: \"XAVIERUNIFORM\" [Weight initialization strategy]\n",
      "                         :     Architecture: \"CPU\" [Which architecture to perform the training on.]\n",
      "                         :     TrainingStrategy: \"LearningRate=1e-1,Momentum=0.9,Repetitions=1,ConvergenceSteps=20,BatchSize=256,TestRepetitions=5,WeightDecay=1e-4,Regularization=None,DropConfig=0.0+0.5+0.5+0.5,\" [Defines the training strategies.]\n",
      "                         : - Default:\n",
      "                         :     VerbosityLevel: \"Default\" [Verbosity level]\n",
      "                         :     CreateMVAPdfs: \"False\" [Create PDFs for classifier outputs (signal and background)]\n",
      "                         :     IgnoreNegWeightsInTraining: \"False\" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]\n"
     ]
    }
   ],
   "source": [
    "if (useCNN) { \n",
    "    TString inputLayoutString(\"InputLayout=1|8|8\");\n",
    "                                                                                                \n",
    "// Batch Layout                                                                                                                                     \n",
    "    TString batchLayoutString(\"BatchLayout=256|1|64\");\n",
    "                                                   \n",
    "\n",
    "TString layoutString(\"Layout=CONV|10|3|3|1|1|1|1|RELU,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,\"\n",
    "                     \"RESHAPE|FLAT,DENSE|64|TANH,DENSE|1|LINEAR\");\n",
    "                                                                                                                                              \n",
    "\n",
    "\n",
    "   // Training strategies.                                                                                                                          \n",
    "   TString training0(\"LearningRate=1e-1,Momentum=0.9,Repetitions=1,\"\n",
    "                     \"ConvergenceSteps=20,BatchSize=256,TestRepetitions=5,\"\n",
    "                     \"WeightDecay=1e-4,Regularization=None,\"\n",
    "                     \"DropConfig=0.0+0.5+0.5+0.5, Multithreading=False\");\n",
    " \n",
    "   TString trainingStrategyString (\"TrainingStrategy=\");\n",
    "   trainingStrategyString += training0; // + \"|\" + training1 + \"|\" + training2;   }\n",
    "    \n",
    "// General Options.                                                                                                                              \n",
    "   TString cnnOptions (\"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:\"\n",
    "                       \"WeightInitialization=XAVIERUNIFORM\");\n",
    "\n",
    "   cnnOptions.Append(\":\"); cnnOptions.Append(inputLayoutString);\n",
    "   cnnOptions.Append(\":\"); cnnOptions.Append(batchLayoutString);\n",
    "   cnnOptions.Append(\":\"); cnnOptions.Append(layoutString);\n",
    "   cnnOptions.Append(\":\"); cnnOptions.Append(trainingStrategyString);\n",
    "   cnnOptions.Append(\":Architecture=CPU\");\n",
    "\n",
    "   //// New DL (CNN)                                                                                                                                \n",
    "\n",
    "\n",
    "  factory.BookMethod(loader, TMVA::Types::kDL, \"DL_CNN_CPU\", cnnOptions);\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book Convolutional Neural Network in Keras using a generated model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (useKeras) { \n",
    "   factory.BookMethod(loader, TMVA::Types::kPyKeras, \n",
    "                       \"PyKeras\",\"H:!V:VarTransform=None:FilenameModel=model_cnn.h5:\"\n",
    "                       \"FilenameTrainedModel=trained_model_cnn.h5:NumEpochs=20:BatchSize=256\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : \u001b[1mTrain all methods\u001b[0m\n",
      "Factory                  : [dataset] : Create Transformation \"I\" with events from all classes.\n",
      "                         : \n",
      "                         : Transformation, Variable selection : \n",
      "                         : Input : variable 'var0' <---> Output : variable 'var0'\n",
      "                         : Input : variable 'var1' <---> Output : variable 'var1'\n",
      "                         : Input : variable 'var2' <---> Output : variable 'var2'\n",
      "                         : Input : variable 'var3' <---> Output : variable 'var3'\n",
      "                         : Input : variable 'var4' <---> Output : variable 'var4'\n",
      "                         : Input : variable 'var5' <---> Output : variable 'var5'\n",
      "                         : Input : variable 'var6' <---> Output : variable 'var6'\n",
      "                         : Input : variable 'var7' <---> Output : variable 'var7'\n",
      "                         : Input : variable 'var8' <---> Output : variable 'var8'\n",
      "                         : Input : variable 'var9' <---> Output : variable 'var9'\n",
      "                         : Input : variable 'var10' <---> Output : variable 'var10'\n",
      "                         : Input : variable 'var11' <---> Output : variable 'var11'\n",
      "                         : Input : variable 'var12' <---> Output : variable 'var12'\n",
      "                         : Input : variable 'var13' <---> Output : variable 'var13'\n",
      "                         : Input : variable 'var14' <---> Output : variable 'var14'\n",
      "                         : Input : variable 'var15' <---> Output : variable 'var15'\n",
      "                         : Input : variable 'var16' <---> Output : variable 'var16'\n",
      "                         : Input : variable 'var17' <---> Output : variable 'var17'\n",
      "                         : Input : variable 'var18' <---> Output : variable 'var18'\n",
      "                         : Input : variable 'var19' <---> Output : variable 'var19'\n",
      "                         : Input : variable 'var20' <---> Output : variable 'var20'\n",
      "                         : Input : variable 'var21' <---> Output : variable 'var21'\n",
      "                         : Input : variable 'var22' <---> Output : variable 'var22'\n",
      "                         : Input : variable 'var23' <---> Output : variable 'var23'\n",
      "                         : Input : variable 'var24' <---> Output : variable 'var24'\n",
      "                         : Input : variable 'var25' <---> Output : variable 'var25'\n",
      "                         : Input : variable 'var26' <---> Output : variable 'var26'\n",
      "                         : Input : variable 'var27' <---> Output : variable 'var27'\n",
      "                         : Input : variable 'var28' <---> Output : variable 'var28'\n",
      "                         : Input : variable 'var29' <---> Output : variable 'var29'\n",
      "                         : Input : variable 'var30' <---> Output : variable 'var30'\n",
      "                         : Input : variable 'var31' <---> Output : variable 'var31'\n",
      "                         : Input : variable 'var32' <---> Output : variable 'var32'\n",
      "                         : Input : variable 'var33' <---> Output : variable 'var33'\n",
      "                         : Input : variable 'var34' <---> Output : variable 'var34'\n",
      "                         : Input : variable 'var35' <---> Output : variable 'var35'\n",
      "                         : Input : variable 'var36' <---> Output : variable 'var36'\n",
      "                         : Input : variable 'var37' <---> Output : variable 'var37'\n",
      "                         : Input : variable 'var38' <---> Output : variable 'var38'\n",
      "                         : Input : variable 'var39' <---> Output : variable 'var39'\n",
      "                         : Input : variable 'var40' <---> Output : variable 'var40'\n",
      "                         : Input : variable 'var41' <---> Output : variable 'var41'\n",
      "                         : Input : variable 'var42' <---> Output : variable 'var42'\n",
      "                         : Input : variable 'var43' <---> Output : variable 'var43'\n",
      "                         : Input : variable 'var44' <---> Output : variable 'var44'\n",
      "                         : Input : variable 'var45' <---> Output : variable 'var45'\n",
      "                         : Input : variable 'var46' <---> Output : variable 'var46'\n",
      "                         : Input : variable 'var47' <---> Output : variable 'var47'\n",
      "                         : Input : variable 'var48' <---> Output : variable 'var48'\n",
      "                         : Input : variable 'var49' <---> Output : variable 'var49'\n",
      "                         : Input : variable 'var50' <---> Output : variable 'var50'\n",
      "                         : Input : variable 'var51' <---> Output : variable 'var51'\n",
      "                         : Input : variable 'var52' <---> Output : variable 'var52'\n",
      "                         : Input : variable 'var53' <---> Output : variable 'var53'\n",
      "                         : Input : variable 'var54' <---> Output : variable 'var54'\n",
      "                         : Input : variable 'var55' <---> Output : variable 'var55'\n",
      "                         : Input : variable 'var56' <---> Output : variable 'var56'\n",
      "                         : Input : variable 'var57' <---> Output : variable 'var57'\n",
      "                         : Input : variable 'var58' <---> Output : variable 'var58'\n",
      "                         : Input : variable 'var59' <---> Output : variable 'var59'\n",
      "                         : Input : variable 'var60' <---> Output : variable 'var60'\n",
      "                         : Input : variable 'var61' <---> Output : variable 'var61'\n",
      "                         : Input : variable 'var62' <---> Output : variable 'var62'\n",
      "                         : Input : variable 'var63' <---> Output : variable 'var63'\n",
      "TFHandler_Factory        : Variable        Mean        RMS   [        Min        Max ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         :     var0:     2.4991     3.8926   [    -10.257     22.057 ]\n",
      "                         :     var1:     3.9787     4.5075   [    -10.122     25.016 ]\n",
      "                         :     var2:     5.5671     4.8848   [    -9.4795     26.928 ]\n",
      "                         :     var3:     6.5379     5.0192   [    -9.7689     29.230 ]\n",
      "                         :     var4:     6.3871     4.9931   [    -11.476     28.503 ]\n",
      "                         :     var5:     5.4269     4.8235   [    -9.1293     30.122 ]\n",
      "                         :     var6:     3.9107     4.4655   [    -11.141     25.933 ]\n",
      "                         :     var7:     2.4388     3.8766   [    -9.8326     22.021 ]\n",
      "                         :     var8:     3.7375     4.4169   [    -9.0364     24.243 ]\n",
      "                         :     var9:     6.0120     5.1559   [    -9.2157     28.235 ]\n",
      "                         :    var10:     8.3558     5.4951   [    -7.3822     29.694 ]\n",
      "                         :    var11:     9.8310     5.5008   [    -7.2496     32.294 ]\n",
      "                         :    var12:     9.7061     5.5019   [    -9.2380     31.239 ]\n",
      "                         :    var13:     8.2453     5.5157   [    -8.3455     31.442 ]\n",
      "                         :    var14:     5.9694     5.0886   [    -9.2506     29.027 ]\n",
      "                         :    var15:     3.6435     4.3955   [    -10.587     23.653 ]\n",
      "                         :    var16:     4.8733     4.7778   [    -11.709     25.905 ]\n",
      "                         :    var17:     7.9688     5.5580   [    -7.5348     33.189 ]\n",
      "                         :    var18:     10.951     5.7539   [    -7.3453     34.627 ]\n",
      "                         :    var19:     12.853     5.4921   [    -4.1117     33.786 ]\n",
      "                         :    var20:     12.820     5.4867   [    -5.3808     32.315 ]\n",
      "                         :    var21:     10.889     5.7651   [    -6.3743     36.072 ]\n",
      "                         :    var22:     7.9053     5.5376   [    -7.9329     32.559 ]\n",
      "                         :    var23:     4.7962     4.7362   [    -10.919     23.587 ]\n",
      "                         :    var24:     5.6555     5.0342   [    -9.8326     29.659 ]\n",
      "                         :    var25:     9.1977     5.6658   [    -6.3310     34.142 ]\n",
      "                         :    var26:     12.554     5.6514   [    -5.9403     33.692 ]\n",
      "                         :    var27:     14.795     5.1093   [    -2.1615     37.430 ]\n",
      "                         :    var28:     14.660     5.2013   [    -2.8283     38.805 ]\n",
      "                         :    var29:     12.449     5.6721   [    -4.6051     37.288 ]\n",
      "                         :    var30:     9.0063     5.7010   [    -10.813     33.124 ]\n",
      "                         :    var31:     5.4970     4.9834   [    -10.336     30.010 ]\n",
      "                         :    var32:     5.6520     5.0125   [    -9.2211     27.428 ]\n",
      "                         :    var33:     9.1908     5.7259   [    -8.0987     33.206 ]\n",
      "                         :    var34:     12.684     5.6427   [    -5.9139     36.323 ]\n",
      "                         :    var35:     14.812     5.1435   [    -3.2133     34.607 ]\n",
      "                         :    var36:     14.707     5.1390   [    -2.8380     35.026 ]\n",
      "                         :    var37:     12.530     5.5950   [    -6.1929     33.746 ]\n",
      "                         :    var38:     9.0409     5.7810   [    -8.0031     32.626 ]\n",
      "                         :    var39:     5.5780     4.9512   [    -8.8630     29.784 ]\n",
      "                         :    var40:     4.9770     4.7758   [    -10.145     25.266 ]\n",
      "                         :    var41:     7.8714     5.5548   [    -8.5314     37.635 ]\n",
      "                         :    var42:     11.016     5.7577   [    -8.1448     36.124 ]\n",
      "                         :    var43:     12.821     5.4932   [    -6.6966     43.293 ]\n",
      "                         :    var44:     12.887     5.5346   [    -4.9059     34.631 ]\n",
      "                         :    var45:     10.988     5.8018   [    -6.8219     34.300 ]\n",
      "                         :    var46:     7.9529     5.5516   [    -10.776     32.909 ]\n",
      "                         :    var47:     4.8126     4.7994   [    -9.0429     27.598 ]\n",
      "                         :    var48:     3.8114     4.3913   [    -10.010     25.702 ]\n",
      "                         :    var49:     6.0972     5.0664   [    -8.8429     30.707 ]\n",
      "                         :    var50:     8.4053     5.4578   [    -8.4494     30.113 ]\n",
      "                         :    var51:     9.8185     5.5505   [    -8.6899     33.313 ]\n",
      "                         :    var52:     9.8044     5.5527   [    -6.3026     33.478 ]\n",
      "                         :    var53:     8.2980     5.4937   [    -8.6684     35.304 ]\n",
      "                         :    var54:     5.9857     5.0800   [    -8.5084     28.695 ]\n",
      "                         :    var55:     3.5967     4.2971   [    -8.7073     21.957 ]\n",
      "                         :    var56:     2.5031     3.9146   [    -11.497     20.361 ]\n",
      "                         :    var57:     3.9950     4.4431   [    -8.8077     25.394 ]\n",
      "                         :    var58:     5.6013     4.8508   [    -11.076     26.883 ]\n",
      "                         :    var59:     6.5296     5.0636   [    -9.0219     28.571 ]\n",
      "                         :    var60:     6.6129     5.0677   [    -9.9130     27.121 ]\n",
      "                         :    var61:     5.5612     4.9134   [    -9.6182     26.950 ]\n",
      "                         :    var62:     4.0187     4.5265   [    -11.569     26.781 ]\n",
      "                         :    var63:     2.4755     3.8720   [    -9.2495     22.465 ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         : \u001b[32m\n",
      "                         : <PlotVariables> Will not produce scatter plots ==> \n",
      "                         : |  The number of 64 input variables and 0 target values would require 2016 two-dimensional\n",
      "                         : |  histograms, which would occupy the computer's memory. Note that this\n",
      "                         : |  suppression does not have any consequences for your analysis, other\n",
      "                         : |  than not disposing of these scatter plots. You can modify the maximum\n",
      "                         : |  number of input variables allowed to generate scatter plots in your\n",
      "                         : |  script via the command line:\n",
      "                         : |  \"(TMVA::gConfig().GetVariablePlotting()).fMaxNumOfAllowedVariablesForScatterPlots = <some int>;\"\u001b[0m\n",
      "                         : \n",
      "                         : Some more output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         : Ranking input variables (method unspecific)...\n",
      "IdTransformation         : Ranking result (top variable is best ranked)\n",
      "                         : ------------------------------\n",
      "                         : Rank : Variable  : Separation\n",
      "                         : ------------------------------\n",
      "                         :    1 : var31     : 1.987e-02\n",
      "                         :    2 : var60     : 1.452e-02\n",
      "                         :    3 : var39     : 1.448e-02\n",
      "                         :    4 : var38     : 1.352e-02\n",
      "                         :    5 : var11     : 1.318e-02\n",
      "                         :    6 : var4      : 1.276e-02\n",
      "                         :    7 : var24     : 1.248e-02\n",
      "                         :    8 : var23     : 1.231e-02\n",
      "                         :    9 : var59     : 1.229e-02\n",
      "                         :   10 : var30     : 1.203e-02\n",
      "                         :   11 : var51     : 1.037e-02\n",
      "                         :   12 : var3      : 9.927e-03\n",
      "                         :   13 : var32     : 9.479e-03\n",
      "                         :   14 : var47     : 8.533e-03\n",
      "                         :   15 : var40     : 8.417e-03\n",
      "                         :   16 : var58     : 8.149e-03\n",
      "                         :   17 : var25     : 8.039e-03\n",
      "                         :   18 : var2      : 8.016e-03\n",
      "                         :   19 : var12     : 7.996e-03\n",
      "                         :   20 : var46     : 7.747e-03\n",
      "                         :   21 : var33     : 7.741e-03\n",
      "                         :   22 : var16     : 7.662e-03\n",
      "                         :   23 : var5      : 7.407e-03\n",
      "                         :   24 : var41     : 6.987e-03\n",
      "                         :   25 : var55     : 6.755e-03\n",
      "                         :   26 : var43     : 6.707e-03\n",
      "                         :   27 : var26     : 6.565e-03\n",
      "                         :   28 : var13     : 6.508e-03\n",
      "                         :   29 : var52     : 6.318e-03\n",
      "                         :   30 : var15     : 6.001e-03\n",
      "                         :   31 : var61     : 5.907e-03\n",
      "                         :   32 : var22     : 5.683e-03\n",
      "                         :   33 : var29     : 5.676e-03\n",
      "                         :   34 : var54     : 5.618e-03\n",
      "                         :   35 : var20     : 5.605e-03\n",
      "                         :   36 : var37     : 5.505e-03\n",
      "                         :   37 : var19     : 5.274e-03\n",
      "                         :   38 : var17     : 4.922e-03\n",
      "                         :   39 : var49     : 4.797e-03\n",
      "                         :   40 : var8      : 4.496e-03\n",
      "                         :   41 : var7      : 4.491e-03\n",
      "                         :   42 : var34     : 4.484e-03\n",
      "                         :   43 : var48     : 4.478e-03\n",
      "                         :   44 : var9      : 4.435e-03\n",
      "                         :   45 : var50     : 4.398e-03\n",
      "                         :   46 : var18     : 4.369e-03\n",
      "                         :   47 : var53     : 4.348e-03\n",
      "                         :   48 : var27     : 4.313e-03\n",
      "                         :   49 : var57     : 4.163e-03\n",
      "                         :   50 : var10     : 4.135e-03\n",
      "                         :   51 : var45     : 4.055e-03\n",
      "                         :   52 : var35     : 3.998e-03\n",
      "                         :   53 : var1      : 3.670e-03\n",
      "                         :   54 : var44     : 3.477e-03\n",
      "                         :   55 : var14     : 3.376e-03\n",
      "                         :   56 : var56     : 3.157e-03\n",
      "                         :   57 : var6      : 3.078e-03\n",
      "                         :   58 : var62     : 2.999e-03\n",
      "                         :   59 : var21     : 2.938e-03\n",
      "                         :   60 : var36     : 2.937e-03\n",
      "                         :   61 : var42     : 2.906e-03\n",
      "                         :   62 : var28     : 2.700e-03\n",
      "                         :   63 : var63     : 2.451e-03\n",
      "                         :   64 : var0      : 2.151e-03\n",
      "                         : ------------------------------\n",
      "Factory                  : Train method: BDT for Classification\n",
      "                         : \n",
      "BDT                      : #events: (reweighted) sig: 5000 bkg: 5000\n",
      "                         : #events: (unweighted) sig: 5000 bkg: 5000\n",
      "                         : Training 800 Decision Trees ... patience please\n",
      "                         : Elapsed time for training with 10000 events: 6.58 sec         \n",
      "BDT                      : [dataset] : Evaluation of BDT on training sample (10000 events)\n",
      "                         : Elapsed time for evaluation of 10000 events: 0.504 sec       \n",
      "                         : Creating xml weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_BDT.weights.xml\u001b[0m\n",
      "                         : Creating standalone class: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_BDT.class.C\u001b[0m\n",
      "Factory                  : Training finished\n",
      "                         : \n",
      "Factory                  : Train method: DNN_CPU for Classification\n",
      "                         : \n",
      "                         : Start of neural network training on CPU.\n",
      "                         : \n",
      "                         : Training phase 1 of 1:\n",
      "                         :      Epoch |   Train Err.  Test  Err.     GFLOP/s Conv. Steps\n",
      "                         : --------------------------------------------------------------\n",
      "                         :          5 |     0.702125    0.693947   0.0284909           0\n",
      "                         :         10 |     0.697657    0.693131   0.0279795           0\n",
      "                         :         15 |     0.697142    0.694839   0.0279532           5\n",
      "                         :         20 |     0.696719    0.695597   0.0280116          10\n",
      "                         :         25 |     0.694152    0.693133   0.0346154          15\n",
      "                         :         30 |     0.692556     0.69151   0.0334327           0\n",
      "                         :         35 |     0.696098    0.695788   0.0337802           5\n",
      "                         :         40 |     0.694712      0.6934   0.0333029          10\n",
      "                         :         45 |     0.691231    0.689955   0.0342574           0\n",
      "                         :         50 |     0.706437    0.704325   0.0359174           5\n",
      "                         :         55 |     0.701161    0.701424   0.0366244          10\n",
      "                         :         60 |     0.693856    0.693122   0.0330877          15\n",
      "                         :         65 |     0.693908    0.693147   0.0331811          20\n",
      "                         : \n",
      "                         : Elapsed time for training with 10000 events: 1.1e+03 sec         \n",
      "DNN_CPU                  : [dataset] : Evaluation of DNN_CPU on training sample (10000 events)\n",
      "                         : Elapsed time for evaluation of 10000 events: 0.386 sec       \n",
      "                         : Creating xml weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_DNN_CPU.weights.xml\u001b[0m\n",
      "                         : Creating standalone class: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_DNN_CPU.class.C\u001b[0m\n",
      "Factory                  : Training finished\n",
      "                         : \n",
      "Factory                  : Train method: DL_CNN_CPU for Classification\n",
      "                         : \n",
      "                         : Start of deep neural network training on CPU.\n",
      "                         : \n",
      "                         : Training phase 1 of 1:\n",
      "*****   Deep Learning Network *****\n",
      "DEEP NEURAL NETWORK:\n",
      "\t Loss function = C\n",
      "\t Network Depth = 6\n",
      "\t Input depth = 1\n",
      "\t Input height = 8\n",
      "\t Input width = 8\n",
      "\t Batch size = 256\n",
      "\t Layers: \n",
      "Layer 0\t CONV LAYER: \t( W = 8 ,  H = 8 ,  D = 10 ) \t Filter ( W = 3 ,  H = 3 ) \tOutput = ( 256 , 10 , 64 ) \t Activation Function = Relu\n",
      "Layer 1\t CONV LAYER: \t( W = 8 ,  H = 8 ,  D = 10 ) \t Filter ( W = 3 ,  H = 3 ) \tOutput = ( 256 , 10 , 64 ) \t Activation Function = Relu\n",
      "Layer 2\t POOL Layer: \t( W = 7 ,  H = 7 ,  D = 10 ) \t Frame ( W = 2 ,  H = 2 ) \tOutput = ( 256 , 10 , 49 ) \n",
      "Layer 3\t RESHAPE Layer \t  Input  ( 10 , 7 , 7 ) \tOutput = ( 1 , 256 , 490 ) \n",
      "Layer 4\t DENSE Layer: \t  ( Input = 490 , Width = 64 ) \tOutput = ( 1 , 256 , 64 ) \t Activation Function = Tanh\n",
      "Layer 5\t DENSE Layer: \t  ( Input = 64 , Width = 1 ) \tOutput = ( 1 , 256 , 1 ) \t Activation Function = Identity\n",
      "                         :      Epoch |   Train Err.  Test  Err.     GFLOP/s   time(s)/epoch Conv. Steps\n",
      "                         : --------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         :          5 |     0.622343    0.629994     8.08344     4.82468           0     20.6478     22.4035     24.1234\n",
      "                         :         10 Minimun Test error found - save the configuration \n",
      "                         :         10 |     0.538763     0.56018     8.02858     4.85764           0     20.5752     22.4787     24.2882\n",
      "                         :         15 Minimun Test error found - save the configuration \n",
      "                         :         15 |     0.512787    0.549101     8.12192     4.80182           0     20.3333     22.1118     24.0091\n",
      "                         :         20 |     0.512692    0.556979      8.2272     4.74037           5     20.4157     22.0872     23.7019\n",
      "                         :         25 |     0.499918    0.563087     8.05309     4.84286          10     20.8029     22.5862     24.2143\n",
      "                         :         30 |     0.487297    0.566336     7.84257     4.97286          15     21.1937     23.0615     24.8643\n",
      "                         :         35 |     0.465178    0.565708      8.0894     4.82112          20     20.9191     22.5038     24.1056\n",
      "                         : \n",
      "                         : Elapsed time for training with 10000 events: 169 sec         \n",
      "DL_CNN_CPU               : [dataset] : Evaluation of DL_CNN_CPU on training sample (10000 events)\n",
      "                         : Elapsed time for evaluation of 10000 events: 2.19 sec       \n",
      "                         : Creating xml weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_DL_CNN_CPU.weights.xml\u001b[0m\n",
      "                         : Creating standalone class: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_DL_CNN_CPU.class.C\u001b[0m\n",
      "Factory                  : Training finished\n",
      "                         : \n",
      "                         : Ranking input variables (method specific)...\n",
      "BDT                      : Ranking result (top variable is best ranked)\n",
      "                         : ---------------------------------------\n",
      "                         : Rank : Variable  : Variable Importance\n",
      "                         : ---------------------------------------\n",
      "                         :    1 : var4      : 2.133e-02\n",
      "                         :    2 : var31     : 2.026e-02\n",
      "                         :    3 : var32     : 2.000e-02\n",
      "                         :    4 : var51     : 1.973e-02\n",
      "                         :    5 : var39     : 1.958e-02\n",
      "                         :    6 : var11     : 1.937e-02\n",
      "                         :    7 : var60     : 1.902e-02\n",
      "                         :    8 : var24     : 1.844e-02\n",
      "                         :    9 : var12     : 1.828e-02\n",
      "                         :   10 : var29     : 1.808e-02\n",
      "                         :   11 : var17     : 1.800e-02\n",
      "                         :   12 : var3      : 1.798e-02\n",
      "                         :   13 : var13     : 1.766e-02\n",
      "                         :   14 : var59     : 1.760e-02\n",
      "                         :   15 : var58     : 1.755e-02\n",
      "                         :   16 : var2      : 1.739e-02\n",
      "                         :   17 : var23     : 1.720e-02\n",
      "                         :   18 : var40     : 1.710e-02\n",
      "                         :   19 : var53     : 1.702e-02\n",
      "                         :   20 : var30     : 1.697e-02\n",
      "                         :   21 : var49     : 1.677e-02\n",
      "                         :   22 : var15     : 1.654e-02\n",
      "                         :   23 : var25     : 1.654e-02\n",
      "                         :   24 : var41     : 1.636e-02\n",
      "                         :   25 : var37     : 1.635e-02\n",
      "                         :   26 : var16     : 1.602e-02\n",
      "                         :   27 : var20     : 1.593e-02\n",
      "                         :   28 : var26     : 1.590e-02\n",
      "                         :   29 : var22     : 1.575e-02\n",
      "                         :   30 : var47     : 1.561e-02\n",
      "                         :   31 : var54     : 1.550e-02\n",
      "                         :   32 : var46     : 1.550e-02\n",
      "                         :   33 : var38     : 1.530e-02\n",
      "                         :   34 : var43     : 1.515e-02\n",
      "                         :   35 : var8      : 1.509e-02\n",
      "                         :   36 : var33     : 1.504e-02\n",
      "                         :   37 : var19     : 1.503e-02\n",
      "                         :   38 : var5      : 1.499e-02\n",
      "                         :   39 : var45     : 1.488e-02\n",
      "                         :   40 : var21     : 1.472e-02\n",
      "                         :   41 : var10     : 1.466e-02\n",
      "                         :   42 : var56     : 1.449e-02\n",
      "                         :   43 : var61     : 1.443e-02\n",
      "                         :   44 : var48     : 1.439e-02\n",
      "                         :   45 : var62     : 1.397e-02\n",
      "                         :   46 : var50     : 1.394e-02\n",
      "                         :   47 : var27     : 1.385e-02\n",
      "                         :   48 : var52     : 1.385e-02\n",
      "                         :   49 : var35     : 1.374e-02\n",
      "                         :   50 : var42     : 1.368e-02\n",
      "                         :   51 : var9      : 1.363e-02\n",
      "                         :   52 : var7      : 1.348e-02\n",
      "                         :   53 : var6      : 1.339e-02\n",
      "                         :   54 : var57     : 1.325e-02\n",
      "                         :   55 : var55     : 1.315e-02\n",
      "                         :   56 : var36     : 1.313e-02\n",
      "                         :   57 : var28     : 1.302e-02\n",
      "                         :   58 : var63     : 1.276e-02\n",
      "                         :   59 : var18     : 1.248e-02\n",
      "                         :   60 : var14     : 1.237e-02\n",
      "                         :   61 : var44     : 1.205e-02\n",
      "                         :   62 : var1      : 1.201e-02\n",
      "                         :   63 : var0      : 1.178e-02\n",
      "                         :   64 : var34     : 1.095e-02\n",
      "                         : ---------------------------------------\n",
      "DNN_CPU                  : Ranking result (top variable is best ranked)\n",
      "                         : ------------------------------\n",
      "                         : Rank : Variable  : Importance\n",
      "                         : ------------------------------\n",
      "                         :    1 : var0      : 1.000e+00\n",
      "                         :    2 : var1      : 1.000e+00\n",
      "                         :    3 : var2      : 1.000e+00\n",
      "                         :    4 : var3      : 1.000e+00\n",
      "                         :    5 : var4      : 1.000e+00\n",
      "                         :    6 : var5      : 1.000e+00\n",
      "                         :    7 : var6      : 1.000e+00\n",
      "                         :    8 : var7      : 1.000e+00\n",
      "                         :    9 : var8      : 1.000e+00\n",
      "                         :   10 : var9      : 1.000e+00\n",
      "                         :   11 : var10     : 1.000e+00\n",
      "                         :   12 : var11     : 1.000e+00\n",
      "                         :   13 : var12     : 1.000e+00\n",
      "                         :   14 : var13     : 1.000e+00\n",
      "                         :   15 : var14     : 1.000e+00\n",
      "                         :   16 : var15     : 1.000e+00\n",
      "                         :   17 : var16     : 1.000e+00\n",
      "                         :   18 : var17     : 1.000e+00\n",
      "                         :   19 : var18     : 1.000e+00\n",
      "                         :   20 : var19     : 1.000e+00\n",
      "                         :   21 : var20     : 1.000e+00\n",
      "                         :   22 : var21     : 1.000e+00\n",
      "                         :   23 : var22     : 1.000e+00\n",
      "                         :   24 : var23     : 1.000e+00\n",
      "                         :   25 : var24     : 1.000e+00\n",
      "                         :   26 : var25     : 1.000e+00\n",
      "                         :   27 : var26     : 1.000e+00\n",
      "                         :   28 : var27     : 1.000e+00\n",
      "                         :   29 : var28     : 1.000e+00\n",
      "                         :   30 : var29     : 1.000e+00\n",
      "                         :   31 : var30     : 1.000e+00\n",
      "                         :   32 : var31     : 1.000e+00\n",
      "                         :   33 : var32     : 1.000e+00\n",
      "                         :   34 : var33     : 1.000e+00\n",
      "                         :   35 : var34     : 1.000e+00\n",
      "                         :   36 : var35     : 1.000e+00\n",
      "                         :   37 : var36     : 1.000e+00\n",
      "                         :   38 : var37     : 1.000e+00\n",
      "                         :   39 : var38     : 1.000e+00\n",
      "                         :   40 : var39     : 1.000e+00\n",
      "                         :   41 : var40     : 1.000e+00\n",
      "                         :   42 : var41     : 1.000e+00\n",
      "                         :   43 : var42     : 1.000e+00\n",
      "                         :   44 : var43     : 1.000e+00\n",
      "                         :   45 : var44     : 1.000e+00\n",
      "                         :   46 : var45     : 1.000e+00\n",
      "                         :   47 : var46     : 1.000e+00\n",
      "                         :   48 : var47     : 1.000e+00\n",
      "                         :   49 : var48     : 1.000e+00\n",
      "                         :   50 : var49     : 1.000e+00\n",
      "                         :   51 : var50     : 1.000e+00\n",
      "                         :   52 : var51     : 1.000e+00\n",
      "                         :   53 : var52     : 1.000e+00\n",
      "                         :   54 : var53     : 1.000e+00\n",
      "                         :   55 : var54     : 1.000e+00\n",
      "                         :   56 : var55     : 1.000e+00\n",
      "                         :   57 : var56     : 1.000e+00\n",
      "                         :   58 : var57     : 1.000e+00\n",
      "                         :   59 : var58     : 1.000e+00\n",
      "                         :   60 : var59     : 1.000e+00\n",
      "                         :   61 : var60     : 1.000e+00\n",
      "                         :   62 : var61     : 1.000e+00\n",
      "                         :   63 : var62     : 1.000e+00\n",
      "                         :   64 : var63     : 1.000e+00\n",
      "                         : ------------------------------\n",
      "                         : No variable ranking supplied by classifier: DL_CNN_CPU\n",
      "Factory                  : === Destroy and recreate all methods via weight files for testing ===\n",
      "                         : \n",
      "                         : Reading weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_BDT.weights.xml\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         : Reading weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_DNN_CPU.weights.xml\u001b[0m\n",
      "                         : Reading weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_DL_CNN_CPU.weights.xml\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "factory.TrainAllMethods();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and Evaluate Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : \u001b[1mTest all methods\u001b[0m\n",
      "Factory                  : Test method: BDT for Classification performance\n",
      "                         : \n",
      "BDT                      : [dataset] : Evaluation of BDT on testing sample (10000 events)\n",
      "                         : Elapsed time for evaluation of 10000 events: 0.425 sec       \n",
      "Factory                  : Test method: DNN_CPU for Classification performance\n",
      "                         : \n",
      "DNN_CPU                  : [dataset] : Evaluation of DNN_CPU on testing sample (10000 events)\n",
      "                         : Elapsed time for evaluation of 10000 events: 0.265 sec       \n",
      "Factory                  : Test method: DL_CNN_CPU for Classification performance\n",
      "                         : \n",
      "DL_CNN_CPU               : [dataset] : Evaluation of DL_CNN_CPU on testing sample (10000 events)\n",
      "                         : Elapsed time for evaluation of 10000 events: 1.75 sec       \n",
      "Factory                  : \u001b[1mEvaluate all methods\u001b[0m\n",
      "Factory                  : Evaluate classifier: BDT\n",
      "                         : \n",
      "BDT                      : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "TFHandler_BDT            : Variable        Mean        RMS   [        Min        Max ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         :     var0:     2.4375     3.8557   [    -9.9918     19.977 ]\n",
      "                         :     var1:     3.9666     4.4410   [    -9.7103     25.867 ]\n",
      "                         :     var2:     5.4753     4.8697   [    -8.5006     29.836 ]\n",
      "                         :     var3:     6.4672     5.0291   [    -7.9837     27.817 ]\n",
      "                         :     var4:     6.5185     5.0298   [    -10.282     27.355 ]\n",
      "                         :     var5:     5.4855     4.8555   [    -7.9858     27.156 ]\n",
      "                         :     var6:     3.9606     4.4945   [    -9.8938     28.512 ]\n",
      "                         :     var7:     2.4387     3.9116   [    -9.9916     21.240 ]\n",
      "                         :     var8:     3.6854     4.3626   [    -9.7199     27.343 ]\n",
      "                         :     var9:     6.0793     5.1200   [    -8.6642     29.457 ]\n",
      "                         :    var10:     8.3184     5.5430   [    -8.5650     31.127 ]\n",
      "                         :    var11:     9.6351     5.5072   [    -7.6700     32.336 ]\n",
      "                         :    var12:     9.6312     5.5191   [    -8.7419     29.683 ]\n",
      "                         :    var13:     8.3128     5.5239   [    -7.2193     36.270 ]\n",
      "                         :    var14:     6.0416     5.1057   [    -9.4517     30.755 ]\n",
      "                         :    var15:     3.6383     4.4067   [    -12.546     22.148 ]\n",
      "                         :    var16:     4.8401     4.7558   [    -11.279     26.243 ]\n",
      "                         :    var17:     7.9437     5.5305   [    -9.1076     32.552 ]\n",
      "                         :    var18:     10.885     5.7504   [    -11.120     35.900 ]\n",
      "                         :    var19:     12.854     5.5008   [    -5.4607     34.438 ]\n",
      "                         :    var20:     12.877     5.5496   [    -4.9795     34.453 ]\n",
      "                         :    var21:     10.946     5.7644   [    -6.3943     38.104 ]\n",
      "                         :    var22:     7.8424     5.4984   [    -7.1890     35.488 ]\n",
      "                         :    var23:     4.8228     4.7835   [    -9.2727     25.210 ]\n",
      "                         :    var24:     5.6459     4.9148   [    -8.6297     25.267 ]\n",
      "                         :    var25:     9.0667     5.6465   [    -7.8932     30.653 ]\n",
      "                         :    var26:     12.553     5.7546   [    -7.4565     38.462 ]\n",
      "                         :    var27:     14.670     5.1246   [    -1.2005     34.769 ]\n",
      "                         :    var28:     14.686     5.1706   [    -1.8391     35.169 ]\n",
      "                         :    var29:     12.466     5.7116   [    -6.3975     36.999 ]\n",
      "                         :    var30:     9.0021     5.7010   [    -8.3901     32.022 ]\n",
      "                         :    var31:     5.6036     5.0045   [    -9.6496     26.932 ]\n",
      "                         :    var32:     5.5822     4.9826   [    -11.560     26.953 ]\n",
      "                         :    var33:     9.1635     5.6928   [    -7.8908     31.500 ]\n",
      "                         :    var34:     12.670     5.6927   [    -8.4293     39.896 ]\n",
      "                         :    var35:     14.712     5.1167   [    -2.6615     34.365 ]\n",
      "                         :    var36:     14.777     5.1516   [    -2.1642     33.879 ]\n",
      "                         :    var37:     12.528     5.6390   [    -7.0200     33.836 ]\n",
      "                         :    var38:     9.0389     5.6715   [    -9.4451     32.219 ]\n",
      "                         :    var39:     5.5557     4.9419   [    -9.8782     27.489 ]\n",
      "                         :    var40:     4.9449     4.8088   [    -10.422     28.788 ]\n",
      "                         :    var41:     7.9424     5.5420   [    -8.4661     31.055 ]\n",
      "                         :    var42:     11.034     5.7483   [    -6.7219     34.394 ]\n",
      "                         :    var43:     12.846     5.4598   [    -6.0367     35.064 ]\n",
      "                         :    var44:     12.885     5.5471   [    -5.3399     36.017 ]\n",
      "                         :    var45:     11.017     5.7452   [    -8.1370     34.427 ]\n",
      "                         :    var46:     7.9332     5.5564   [    -9.6217     31.459 ]\n",
      "                         :    var47:     4.9462     4.8152   [    -9.0932     24.864 ]\n",
      "                         :    var48:     3.7166     4.3715   [    -10.844     25.228 ]\n",
      "                         :    var49:     6.1139     5.1265   [    -8.5402     29.845 ]\n",
      "                         :    var50:     8.3390     5.4683   [    -8.2915     32.722 ]\n",
      "                         :    var51:     9.8672     5.5727   [    -8.3201     33.142 ]\n",
      "                         :    var52:     9.8495     5.5322   [    -8.3466     33.869 ]\n",
      "                         :    var53:     8.4185     5.4828   [    -9.7019     33.503 ]\n",
      "                         :    var54:     6.0872     5.1308   [    -9.1295     32.443 ]\n",
      "                         :    var55:     3.7002     4.3962   [    -10.585     27.256 ]\n",
      "                         :    var56:     2.4642     3.9476   [    -9.7419     22.620 ]\n",
      "                         :    var57:     4.0422     4.5454   [    -8.2481     26.215 ]\n",
      "                         :    var58:     5.5688     4.8957   [    -7.9015     30.791 ]\n",
      "                         :    var59:     6.5518     5.0494   [    -8.9191     29.350 ]\n",
      "                         :    var60:     6.5388     5.0481   [    -8.4943     28.476 ]\n",
      "                         :    var61:     5.6626     4.9085   [    -9.0860     29.674 ]\n",
      "                         :    var62:     4.0584     4.5142   [    -9.4862     23.124 ]\n",
      "                         :    var63:     2.5276     3.9035   [    -10.527     21.548 ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         : \u001b[32m\n",
      "                         : <PlotVariables> Will not produce scatter plots ==> \n",
      "                         : |  The number of 64 input variables and 0 target values would require 2016 two-dimensional\n",
      "                         : |  histograms, which would occupy the computer's memory. Note that this\n",
      "                         : |  suppression does not have any consequences for your analysis, other\n",
      "                         : |  than not disposing of these scatter plots. You can modify the maximum\n",
      "                         : |  number of input variables allowed to generate scatter plots in your\n",
      "                         : |  script via the command line:\n",
      "                         : |  \"(TMVA::gConfig().GetVariablePlotting()).fMaxNumOfAllowedVariablesForScatterPlots = <some int>;\"\u001b[0m\n",
      "                         : \n",
      "                         : Some more output\n",
      "Factory                  : Evaluate classifier: DNN_CPU\n",
      "                         : \n",
      "DNN_CPU                  : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.005, fb=-0.005), refValue = 0.005\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.015, fb=-0.015), refValue = 0.015\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.025, fb=-0.025), refValue = 0.025\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.035, fb=-0.035), refValue = 0.035\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.045, fb=-0.045), refValue = 0.045\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.055, fb=-0.055), refValue = 0.055\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.065, fb=-0.065), refValue = 0.065\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.075, fb=-0.075), refValue = 0.075\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.085, fb=-0.085), refValue = 0.085\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.095, fb=-0.095), refValue = 0.095\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.105, fb=-0.105), refValue = 0.105\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.115, fb=-0.115), refValue = 0.115\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.125, fb=-0.125), refValue = 0.125\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.135, fb=-0.135), refValue = 0.135\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.145, fb=-0.145), refValue = 0.145\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.155, fb=-0.155), refValue = 0.155\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.165, fb=-0.165), refValue = 0.165\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.175, fb=-0.175), refValue = 0.175\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.185, fb=-0.185), refValue = 0.185\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.195, fb=-0.195), refValue = 0.195\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.205, fb=-0.205), refValue = 0.205\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.215, fb=-0.215), refValue = 0.215\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.225, fb=-0.225), refValue = 0.225\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.235, fb=-0.235), refValue = 0.235\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.245, fb=-0.245), refValue = 0.245\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.255, fb=-0.255), refValue = 0.255\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.265, fb=-0.265), refValue = 0.265\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.275, fb=-0.275), refValue = 0.275\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.285, fb=-0.285), refValue = 0.285\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.295, fb=-0.295), refValue = 0.295\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.305, fb=-0.305), refValue = 0.305\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.315, fb=-0.315), refValue = 0.315\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.325, fb=-0.325), refValue = 0.325\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.335, fb=-0.335), refValue = 0.335\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.345, fb=-0.345), refValue = 0.345\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.355, fb=-0.355), refValue = 0.355\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.365, fb=-0.365), refValue = 0.365\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.375, fb=-0.375), refValue = 0.375\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.385, fb=-0.385), refValue = 0.385\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.395, fb=-0.395), refValue = 0.395\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.405, fb=-0.405), refValue = 0.405\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.415, fb=-0.415), refValue = 0.415\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.425, fb=-0.425), refValue = 0.425\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.435, fb=-0.435), refValue = 0.435\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.445, fb=-0.445), refValue = 0.445\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.455, fb=-0.455), refValue = 0.455\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.465, fb=-0.465), refValue = 0.465\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.475, fb=-0.475), refValue = 0.475\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.485, fb=-0.485), refValue = 0.485\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.495, fb=-0.495), refValue = 0.495\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.505, fb=-0.505), refValue = 0.505\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.515, fb=-0.515), refValue = 0.515\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.525, fb=-0.525), refValue = 0.525\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.535, fb=-0.535), refValue = 0.535\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.545, fb=-0.545), refValue = 0.545\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.555, fb=-0.555), refValue = 0.555\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.565, fb=-0.565), refValue = 0.565\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.575, fb=-0.575), refValue = 0.575\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.585, fb=-0.585), refValue = 0.585\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.595, fb=-0.595), refValue = 0.595\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.605, fb=-0.605), refValue = 0.605\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.615, fb=-0.615), refValue = 0.615\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.625, fb=-0.625), refValue = 0.625\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.635, fb=-0.635), refValue = 0.635\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.645, fb=-0.645), refValue = 0.645\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.655, fb=-0.655), refValue = 0.655\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.665, fb=-0.665), refValue = 0.665\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.675, fb=-0.675), refValue = 0.675\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.685, fb=-0.685), refValue = 0.685\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.695, fb=-0.695), refValue = 0.695\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.705, fb=-0.705), refValue = 0.705\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.715, fb=-0.715), refValue = 0.715\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.725, fb=-0.725), refValue = 0.725\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.735, fb=-0.735), refValue = 0.735\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.745, fb=-0.745), refValue = 0.745\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.755, fb=-0.755), refValue = 0.755\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.765, fb=-0.765), refValue = 0.765\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.775, fb=-0.775), refValue = 0.775\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.785, fb=-0.785), refValue = 0.785\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.795, fb=-0.795), refValue = 0.795\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.805, fb=-0.805), refValue = 0.805\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.815, fb=-0.815), refValue = 0.815\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.825, fb=-0.825), refValue = 0.825\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.835, fb=-0.835), refValue = 0.835\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.845, fb=-0.845), refValue = 0.845\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.855, fb=-0.855), refValue = 0.855\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.865, fb=-0.865), refValue = 0.865\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.875, fb=-0.875), refValue = 0.875\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.885, fb=-0.885), refValue = 0.885\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.895, fb=-0.895), refValue = 0.895\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.905, fb=-0.905), refValue = 0.905\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.915, fb=-0.915), refValue = 0.915\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.925, fb=-0.925), refValue = 0.925\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.935, fb=-0.935), refValue = 0.935\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.945, fb=-0.945), refValue = 0.945\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.955, fb=-0.955), refValue = 0.955\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.965, fb=-0.965), refValue = 0.965\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.975, fb=-0.975), refValue = 0.975\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.985, fb=-0.985), refValue = 0.985\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.995, fb=-0.995), refValue = 0.995\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.9999, fb=-0.9999), refValue = 0.9999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.005, fb=-0.005), refValue = 0.005\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.015, fb=-0.015), refValue = 0.015\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.025, fb=-0.025), refValue = 0.025\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.035, fb=-0.035), refValue = 0.035\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.045, fb=-0.045), refValue = 0.045\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.055, fb=-0.055), refValue = 0.055\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.065, fb=-0.065), refValue = 0.065\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.075, fb=-0.075), refValue = 0.075\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.085, fb=-0.085), refValue = 0.085\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.095, fb=-0.095), refValue = 0.095\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.105, fb=-0.105), refValue = 0.105\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.115, fb=-0.115), refValue = 0.115\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.125, fb=-0.125), refValue = 0.125\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.135, fb=-0.135), refValue = 0.135\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.145, fb=-0.145), refValue = 0.145\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.155, fb=-0.155), refValue = 0.155\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.165, fb=-0.165), refValue = 0.165\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.175, fb=-0.175), refValue = 0.175\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.185, fb=-0.185), refValue = 0.185\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.195, fb=-0.195), refValue = 0.195\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.205, fb=-0.205), refValue = 0.205\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.215, fb=-0.215), refValue = 0.215\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.225, fb=-0.225), refValue = 0.225\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.235, fb=-0.235), refValue = 0.235\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.245, fb=-0.245), refValue = 0.245\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.255, fb=-0.255), refValue = 0.255\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.265, fb=-0.265), refValue = 0.265\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.275, fb=-0.275), refValue = 0.275\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.285, fb=-0.285), refValue = 0.285\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.295, fb=-0.295), refValue = 0.295\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.305, fb=-0.305), refValue = 0.305\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.315, fb=-0.315), refValue = 0.315\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.325, fb=-0.325), refValue = 0.325\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.335, fb=-0.335), refValue = 0.335\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.345, fb=-0.345), refValue = 0.345\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.355, fb=-0.355), refValue = 0.355\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.365, fb=-0.365), refValue = 0.365\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.375, fb=-0.375), refValue = 0.375\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.385, fb=-0.385), refValue = 0.385\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.395, fb=-0.395), refValue = 0.395\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.405, fb=-0.405), refValue = 0.405\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.415, fb=-0.415), refValue = 0.415\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.425, fb=-0.425), refValue = 0.425\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.435, fb=-0.435), refValue = 0.435\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.445, fb=-0.445), refValue = 0.445\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.455, fb=-0.455), refValue = 0.455\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.465, fb=-0.465), refValue = 0.465\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.475, fb=-0.475), refValue = 0.475\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.485, fb=-0.485), refValue = 0.485\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.495, fb=-0.495), refValue = 0.495\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.505, fb=-0.505), refValue = 0.505\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.515, fb=-0.515), refValue = 0.515\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.525, fb=-0.525), refValue = 0.525\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.535, fb=-0.535), refValue = 0.535\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.545, fb=-0.545), refValue = 0.545\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.555, fb=-0.555), refValue = 0.555\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.565, fb=-0.565), refValue = 0.565\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.575, fb=-0.575), refValue = 0.575\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.585, fb=-0.585), refValue = 0.585\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.595, fb=-0.595), refValue = 0.595\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.605, fb=-0.605), refValue = 0.605\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.615, fb=-0.615), refValue = 0.615\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.625, fb=-0.625), refValue = 0.625\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.635, fb=-0.635), refValue = 0.635\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.645, fb=-0.645), refValue = 0.645\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.655, fb=-0.655), refValue = 0.655\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.665, fb=-0.665), refValue = 0.665\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.675, fb=-0.675), refValue = 0.675\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.685, fb=-0.685), refValue = 0.685\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.695, fb=-0.695), refValue = 0.695\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.705, fb=-0.705), refValue = 0.705\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.715, fb=-0.715), refValue = 0.715\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.725, fb=-0.725), refValue = 0.725\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.735, fb=-0.735), refValue = 0.735\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.745, fb=-0.745), refValue = 0.745\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.755, fb=-0.755), refValue = 0.755\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.765, fb=-0.765), refValue = 0.765\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.775, fb=-0.775), refValue = 0.775\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.785, fb=-0.785), refValue = 0.785\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.795, fb=-0.795), refValue = 0.795\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.805, fb=-0.805), refValue = 0.805\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.815, fb=-0.815), refValue = 0.815\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.825, fb=-0.825), refValue = 0.825\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.835, fb=-0.835), refValue = 0.835\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.845, fb=-0.845), refValue = 0.845\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.855, fb=-0.855), refValue = 0.855\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.865, fb=-0.865), refValue = 0.865\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.875, fb=-0.875), refValue = 0.875\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.885, fb=-0.885), refValue = 0.885\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.895, fb=-0.895), refValue = 0.895\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.905, fb=-0.905), refValue = 0.905\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.915, fb=-0.915), refValue = 0.915\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.925, fb=-0.925), refValue = 0.925\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.935, fb=-0.935), refValue = 0.935\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.945, fb=-0.945), refValue = 0.945\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.955, fb=-0.955), refValue = 0.955\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.965, fb=-0.965), refValue = 0.965\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.975, fb=-0.975), refValue = 0.975\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.985, fb=-0.985), refValue = 0.985\n",
      "<WARNING>                : <Root> initial interval w/o root: (a=0.493992, b=0.493994), (Eff_a=0, Eff_b=0), (fa=-0.995, fb=-0.995), refValue = 0.995\n",
      "TFHandler_DNN_CPU        : Variable        Mean        RMS   [        Min        Max ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         :     var0:     2.4375     3.8557   [    -9.9918     19.977 ]\n",
      "                         :     var1:     3.9666     4.4410   [    -9.7103     25.867 ]\n",
      "                         :     var2:     5.4753     4.8697   [    -8.5006     29.836 ]\n",
      "                         :     var3:     6.4672     5.0291   [    -7.9837     27.817 ]\n",
      "                         :     var4:     6.5185     5.0298   [    -10.282     27.355 ]\n",
      "                         :     var5:     5.4855     4.8555   [    -7.9858     27.156 ]\n",
      "                         :     var6:     3.9606     4.4945   [    -9.8938     28.512 ]\n",
      "                         :     var7:     2.4387     3.9116   [    -9.9916     21.240 ]\n",
      "                         :     var8:     3.6854     4.3626   [    -9.7199     27.343 ]\n",
      "                         :     var9:     6.0793     5.1200   [    -8.6642     29.457 ]\n",
      "                         :    var10:     8.3184     5.5430   [    -8.5650     31.127 ]\n",
      "                         :    var11:     9.6351     5.5072   [    -7.6700     32.336 ]\n",
      "                         :    var12:     9.6312     5.5191   [    -8.7419     29.683 ]\n",
      "                         :    var13:     8.3128     5.5239   [    -7.2193     36.270 ]\n",
      "                         :    var14:     6.0416     5.1057   [    -9.4517     30.755 ]\n",
      "                         :    var15:     3.6383     4.4067   [    -12.546     22.148 ]\n",
      "                         :    var16:     4.8401     4.7558   [    -11.279     26.243 ]\n",
      "                         :    var17:     7.9437     5.5305   [    -9.1076     32.552 ]\n",
      "                         :    var18:     10.885     5.7504   [    -11.120     35.900 ]\n",
      "                         :    var19:     12.854     5.5008   [    -5.4607     34.438 ]\n",
      "                         :    var20:     12.877     5.5496   [    -4.9795     34.453 ]\n",
      "                         :    var21:     10.946     5.7644   [    -6.3943     38.104 ]\n",
      "                         :    var22:     7.8424     5.4984   [    -7.1890     35.488 ]\n",
      "                         :    var23:     4.8228     4.7835   [    -9.2727     25.210 ]\n",
      "                         :    var24:     5.6459     4.9148   [    -8.6297     25.267 ]\n",
      "                         :    var25:     9.0667     5.6465   [    -7.8932     30.653 ]\n",
      "                         :    var26:     12.553     5.7546   [    -7.4565     38.462 ]\n",
      "                         :    var27:     14.670     5.1246   [    -1.2005     34.769 ]\n",
      "                         :    var28:     14.686     5.1706   [    -1.8391     35.169 ]\n",
      "                         :    var29:     12.466     5.7116   [    -6.3975     36.999 ]\n",
      "                         :    var30:     9.0021     5.7010   [    -8.3901     32.022 ]\n",
      "                         :    var31:     5.6036     5.0045   [    -9.6496     26.932 ]\n",
      "                         :    var32:     5.5822     4.9826   [    -11.560     26.953 ]\n",
      "                         :    var33:     9.1635     5.6928   [    -7.8908     31.500 ]\n",
      "                         :    var34:     12.670     5.6927   [    -8.4293     39.896 ]\n",
      "                         :    var35:     14.712     5.1167   [    -2.6615     34.365 ]\n",
      "                         :    var36:     14.777     5.1516   [    -2.1642     33.879 ]\n",
      "                         :    var37:     12.528     5.6390   [    -7.0200     33.836 ]\n",
      "                         :    var38:     9.0389     5.6715   [    -9.4451     32.219 ]\n",
      "                         :    var39:     5.5557     4.9419   [    -9.8782     27.489 ]\n",
      "                         :    var40:     4.9449     4.8088   [    -10.422     28.788 ]\n",
      "                         :    var41:     7.9424     5.5420   [    -8.4661     31.055 ]\n",
      "                         :    var42:     11.034     5.7483   [    -6.7219     34.394 ]\n",
      "                         :    var43:     12.846     5.4598   [    -6.0367     35.064 ]\n",
      "                         :    var44:     12.885     5.5471   [    -5.3399     36.017 ]\n",
      "                         :    var45:     11.017     5.7452   [    -8.1370     34.427 ]\n",
      "                         :    var46:     7.9332     5.5564   [    -9.6217     31.459 ]\n",
      "                         :    var47:     4.9462     4.8152   [    -9.0932     24.864 ]\n",
      "                         :    var48:     3.7166     4.3715   [    -10.844     25.228 ]\n",
      "                         :    var49:     6.1139     5.1265   [    -8.5402     29.845 ]\n",
      "                         :    var50:     8.3390     5.4683   [    -8.2915     32.722 ]\n",
      "                         :    var51:     9.8672     5.5727   [    -8.3201     33.142 ]\n",
      "                         :    var52:     9.8495     5.5322   [    -8.3466     33.869 ]\n",
      "                         :    var53:     8.4185     5.4828   [    -9.7019     33.503 ]\n",
      "                         :    var54:     6.0872     5.1308   [    -9.1295     32.443 ]\n",
      "                         :    var55:     3.7002     4.3962   [    -10.585     27.256 ]\n",
      "                         :    var56:     2.4642     3.9476   [    -9.7419     22.620 ]\n",
      "                         :    var57:     4.0422     4.5454   [    -8.2481     26.215 ]\n",
      "                         :    var58:     5.5688     4.8957   [    -7.9015     30.791 ]\n",
      "                         :    var59:     6.5518     5.0494   [    -8.9191     29.350 ]\n",
      "                         :    var60:     6.5388     5.0481   [    -8.4943     28.476 ]\n",
      "                         :    var61:     5.6626     4.9085   [    -9.0860     29.674 ]\n",
      "                         :    var62:     4.0584     4.5142   [    -9.4862     23.124 ]\n",
      "                         :    var63:     2.5276     3.9035   [    -10.527     21.548 ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         : \u001b[32m\n",
      "                         : <PlotVariables> Will not produce scatter plots ==> \n",
      "                         : |  The number of 64 input variables and 0 target values would require 2016 two-dimensional\n",
      "                         : |  histograms, which would occupy the computer's memory. Note that this\n",
      "                         : |  suppression does not have any consequences for your analysis, other\n",
      "                         : |  than not disposing of these scatter plots. You can modify the maximum\n",
      "                         : |  number of input variables allowed to generate scatter plots in your\n",
      "                         : |  script via the command line:\n",
      "                         : |  \"(TMVA::gConfig().GetVariablePlotting()).fMaxNumOfAllowedVariablesForScatterPlots = <some int>;\"\u001b[0m\n",
      "                         : \n",
      "                         : Some more output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : Evaluate classifier: DL_CNN_CPU\n",
      "                         : \n",
      "DL_CNN_CPU               : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "TFHandler_DL_CNN_CPU     : Variable        Mean        RMS   [        Min        Max ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         :     var0:     2.4375     3.8557   [    -9.9918     19.977 ]\n",
      "                         :     var1:     3.9666     4.4410   [    -9.7103     25.867 ]\n",
      "                         :     var2:     5.4753     4.8697   [    -8.5006     29.836 ]\n",
      "                         :     var3:     6.4672     5.0291   [    -7.9837     27.817 ]\n",
      "                         :     var4:     6.5185     5.0298   [    -10.282     27.355 ]\n",
      "                         :     var5:     5.4855     4.8555   [    -7.9858     27.156 ]\n",
      "                         :     var6:     3.9606     4.4945   [    -9.8938     28.512 ]\n",
      "                         :     var7:     2.4387     3.9116   [    -9.9916     21.240 ]\n",
      "                         :     var8:     3.6854     4.3626   [    -9.7199     27.343 ]\n",
      "                         :     var9:     6.0793     5.1200   [    -8.6642     29.457 ]\n",
      "                         :    var10:     8.3184     5.5430   [    -8.5650     31.127 ]\n",
      "                         :    var11:     9.6351     5.5072   [    -7.6700     32.336 ]\n",
      "                         :    var12:     9.6312     5.5191   [    -8.7419     29.683 ]\n",
      "                         :    var13:     8.3128     5.5239   [    -7.2193     36.270 ]\n",
      "                         :    var14:     6.0416     5.1057   [    -9.4517     30.755 ]\n",
      "                         :    var15:     3.6383     4.4067   [    -12.546     22.148 ]\n",
      "                         :    var16:     4.8401     4.7558   [    -11.279     26.243 ]\n",
      "                         :    var17:     7.9437     5.5305   [    -9.1076     32.552 ]\n",
      "                         :    var18:     10.885     5.7504   [    -11.120     35.900 ]\n",
      "                         :    var19:     12.854     5.5008   [    -5.4607     34.438 ]\n",
      "                         :    var20:     12.877     5.5496   [    -4.9795     34.453 ]\n",
      "                         :    var21:     10.946     5.7644   [    -6.3943     38.104 ]\n",
      "                         :    var22:     7.8424     5.4984   [    -7.1890     35.488 ]\n",
      "                         :    var23:     4.8228     4.7835   [    -9.2727     25.210 ]\n",
      "                         :    var24:     5.6459     4.9148   [    -8.6297     25.267 ]\n",
      "                         :    var25:     9.0667     5.6465   [    -7.8932     30.653 ]\n",
      "                         :    var26:     12.553     5.7546   [    -7.4565     38.462 ]\n",
      "                         :    var27:     14.670     5.1246   [    -1.2005     34.769 ]\n",
      "                         :    var28:     14.686     5.1706   [    -1.8391     35.169 ]\n",
      "                         :    var29:     12.466     5.7116   [    -6.3975     36.999 ]\n",
      "                         :    var30:     9.0021     5.7010   [    -8.3901     32.022 ]\n",
      "                         :    var31:     5.6036     5.0045   [    -9.6496     26.932 ]\n",
      "                         :    var32:     5.5822     4.9826   [    -11.560     26.953 ]\n",
      "                         :    var33:     9.1635     5.6928   [    -7.8908     31.500 ]\n",
      "                         :    var34:     12.670     5.6927   [    -8.4293     39.896 ]\n",
      "                         :    var35:     14.712     5.1167   [    -2.6615     34.365 ]\n",
      "                         :    var36:     14.777     5.1516   [    -2.1642     33.879 ]\n",
      "                         :    var37:     12.528     5.6390   [    -7.0200     33.836 ]\n",
      "                         :    var38:     9.0389     5.6715   [    -9.4451     32.219 ]\n",
      "                         :    var39:     5.5557     4.9419   [    -9.8782     27.489 ]\n",
      "                         :    var40:     4.9449     4.8088   [    -10.422     28.788 ]\n",
      "                         :    var41:     7.9424     5.5420   [    -8.4661     31.055 ]\n",
      "                         :    var42:     11.034     5.7483   [    -6.7219     34.394 ]\n",
      "                         :    var43:     12.846     5.4598   [    -6.0367     35.064 ]\n",
      "                         :    var44:     12.885     5.5471   [    -5.3399     36.017 ]\n",
      "                         :    var45:     11.017     5.7452   [    -8.1370     34.427 ]\n",
      "                         :    var46:     7.9332     5.5564   [    -9.6217     31.459 ]\n",
      "                         :    var47:     4.9462     4.8152   [    -9.0932     24.864 ]\n",
      "                         :    var48:     3.7166     4.3715   [    -10.844     25.228 ]\n",
      "                         :    var49:     6.1139     5.1265   [    -8.5402     29.845 ]\n",
      "                         :    var50:     8.3390     5.4683   [    -8.2915     32.722 ]\n",
      "                         :    var51:     9.8672     5.5727   [    -8.3201     33.142 ]\n",
      "                         :    var52:     9.8495     5.5322   [    -8.3466     33.869 ]\n",
      "                         :    var53:     8.4185     5.4828   [    -9.7019     33.503 ]\n",
      "                         :    var54:     6.0872     5.1308   [    -9.1295     32.443 ]\n",
      "                         :    var55:     3.7002     4.3962   [    -10.585     27.256 ]\n",
      "                         :    var56:     2.4642     3.9476   [    -9.7419     22.620 ]\n",
      "                         :    var57:     4.0422     4.5454   [    -8.2481     26.215 ]\n",
      "                         :    var58:     5.5688     4.8957   [    -7.9015     30.791 ]\n",
      "                         :    var59:     6.5518     5.0494   [    -8.9191     29.350 ]\n",
      "                         :    var60:     6.5388     5.0481   [    -8.4943     28.476 ]\n",
      "                         :    var61:     5.6626     4.9085   [    -9.0860     29.674 ]\n",
      "                         :    var62:     4.0584     4.5142   [    -9.4862     23.124 ]\n",
      "                         :    var63:     2.5276     3.9035   [    -10.527     21.548 ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         : \u001b[32m\n",
      "                         : <PlotVariables> Will not produce scatter plots ==> \n",
      "                         : |  The number of 64 input variables and 0 target values would require 2016 two-dimensional\n",
      "                         : |  histograms, which would occupy the computer's memory. Note that this\n",
      "                         : |  suppression does not have any consequences for your analysis, other\n",
      "                         : |  than not disposing of these scatter plots. You can modify the maximum\n",
      "                         : |  number of input variables allowed to generate scatter plots in your\n",
      "                         : |  script via the command line:\n",
      "                         : |  \"(TMVA::gConfig().GetVariablePlotting()).fMaxNumOfAllowedVariablesForScatterPlots = <some int>;\"\u001b[0m\n",
      "                         : \n",
      "                         : Some more output\n",
      "                         : \n",
      "                         : Evaluation results ranked by best signal efficiency and purity (area)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet       MVA                       \n",
      "                         : Name:         Method:          ROC-integ\n",
      "                         : dataset       DL_CNN_CPU     : 0.795\n",
      "                         : dataset       BDT            : 0.787\n",
      "                         : dataset       DNN_CPU        : 0.422\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : Testing efficiency compared to training efficiency (overtraining check)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) \n",
      "                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   \n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : dataset              DL_CNN_CPU     : 0.120 (0.133)       0.447 (0.499)      0.743 (0.778)\n",
      "                         : dataset              BDT            : 0.115 (0.195)       0.443 (0.536)      0.728 (0.781)\n",
      "                         : dataset              DNN_CPU        : 0.000 (0.000)       0.000 (0.000)      0.000 (0.000)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:dataset          : Created tree 'TestTree' with 10000 events\n",
      "                         : \n",
      "Dataset:dataset          : Created tree 'TrainTree' with 10000 events\n",
      "                         : \n",
      "Factory                  : \u001b[1mThank you for using TMVA!\u001b[0m\n",
      "                         : \u001b[1mFor citation information, please visit: http://tmva.sf.net/citeTMVA.html\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "factory.TestAllMethods();\n",
    "factory.EvaluateAllMethods();    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC Curve\n",
    "We enable JavaScript visualisation for the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%jsroot on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArgAAAHYCAIAAAApvgy/AAAABmJLR0QAAAAAAAD5Q7t/AAAgAElE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\nonbqjCaFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c1 = factory.GetROCCurve(loader);\n",
    "c1->Draw();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// close outputfile to save output file\n",
    "outputFile->Close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ROOT C++",
   "language": "c++",
   "name": "root"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".C",
   "mimetype": " text/x-c++src",
   "name": "c++"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
